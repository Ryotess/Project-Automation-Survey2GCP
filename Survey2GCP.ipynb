{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c24efb16",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18544543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import zipfile\n",
    "import pyreadstat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "class bcolors:\n",
    "    YELLOW = '\\033[93m'\n",
    "    BOLD = '\\033[01m'\n",
    "    GREEN = '\\033[92m'\n",
    "    RED = '\\033[91m'\n",
    "    ENDC = '\\033[0m'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a056ca3a",
   "metadata": {},
   "source": [
    "# Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input path | Surveys Raw data (.zip format)\n",
    "input_path = './input/'\n",
    "\n",
    "## output and csv folder path\n",
    "output_path = './output/' # For unzipped .sav files\n",
    "csv_path = './csv_file/' # For Preprocessed .csv files\n",
    "\n",
    "## Credentials for GCP\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./credentials/Your credential files' name\" # Add credentials to environment\n",
    "\n",
    "## Project and dataset to upload\n",
    "PROJECT_ID = 'project-id' # Put your project id here\n",
    "dataset_id = 'dataset-id' # dataset to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbe868",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create output and csv folder if not exist\n",
    "if not os.path.exists(output_path):\n",
    "   os.makedirs(output_path)\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "   os.makedirs(csv_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b48fbca9",
   "metadata": {},
   "source": [
    "# Part A. Unzip and Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4ceb325",
   "metadata": {},
   "source": [
    "## 1. Unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0102774",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(input_path)\n",
    "for d in dir_list:\n",
    "    if('.zip' in d):\n",
    "        zip_file = zipfile.ZipFile(input_path+d)\n",
    "        file_name = d.replace(\"zip\",\"sav\")\n",
    "        zipinfos = zip_file.infolist()\n",
    "        zipinfos[0].filename = file_name\n",
    "        zip_file.extract(zipinfos[0], path = output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c599e7e9",
   "metadata": {},
   "source": [
    "## 2. Preprocess/Analytics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5445a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Proces_func(df, your parameters here):\n",
    "    \"\"\"define your processing function here\"\"\"\n",
    "    return df_processed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4c7a2f2",
   "metadata": {},
   "source": [
    "## 3. Export to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(output_path):\n",
    "    if('.sav' in file):\n",
    "        print(f\"{bcolors.YELLOW}Process file: {bcolors.ENDC}\",file)\n",
    "\n",
    "\n",
    "        try:\n",
    "            df, meta = pyreadstat.read_sav(output_path + file) # read survey raw data\n",
    "            new_name = \"you can use regular expression or your naming rule to create a new name\" # name for output .csv\n",
    "            # # extract spss variables(optional: if your analytics need these info)\n",
    "            # labels_dict = meta.column_names_to_labels\n",
    "            # value_labels_dict = meta.variable_value_labels\n",
    "            # measure_dict = meta.variable_measure\n",
    "            # variable_type = meta.original_variable_types\n",
    "\n",
    "            # Preprocessing/Analytics function\n",
    "            Proces_func()\n",
    "\n",
    "            # # You may want to save a new version of processed SAV file,\n",
    "            # # remember if you change the variables/columns during the preprocessing function,\n",
    "            # # those saving information like variable_format/column_labels... also need to be modified.\n",
    "            # pyreadstat.write_sav(df_processed, f'{output_path}{file}_processed',\n",
    "            #                     variable_format=variable_type ,\n",
    "            #                     column_labels=labels_dict ,\n",
    "            #                     variable_value_labels=value_labels_dict,\n",
    "            #                     variable_measure=measure_dict,\n",
    "            #                     row_compress=True)\n",
    "            \n",
    "\n",
    "            df_processed.to_csv(f'{csv_path}{new_name}.csv', index=False)\n",
    "            print(f'{bcolors.BOLD}===>{bcolors.ENDC} {csv_path}{new_name}.csv :{bcolors.GREEN}Done{bcolors.ENDC}.')\n",
    "            \n",
    "        except:\n",
    "            print(f'{bcolors.BOLD}===>{bcolors.ENDC} {csv_path}{new_name}.csv :{bcolors.RED}Failed{bcolors.ENDC}.')\n",
    "\n",
    "        print('---')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05d016e9",
   "metadata": {},
   "source": [
    "# Part B. Upload to GCP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef85a481",
   "metadata": {},
   "source": [
    "## 1. Upload Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a368a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GCP_upload(PROJECT_ID, csv_file, dataset_id):\n",
    "    ## BigQuery package\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    ## create a client instance for your project\n",
    "    client = bigquery.Client(project=PROJECT_ID, location=\"US\")\n",
    "\n",
    "    ## Upload files in folderpath\n",
    "    filelist = os.listdir(csv_file) # get all files' name in folderpath\n",
    "\n",
    "    for file in filelist:\n",
    "        if file.endswith('.csv'):\n",
    "            filename = f'{csv_file}{file}' # file path\n",
    "            table_id = file[:-4:] # use file name (remove .csv) as table name\n",
    "\n",
    "            #check if the table exists\n",
    "            tables = client.list_tables(dataset_id)\n",
    "            table_list = [table.table_id for table in tables]\n",
    "            if not table_id in table_list:\n",
    "                client.create_table(f\"{PROJECT_ID}.{dataset_id}.{table_id}\") # create table to store data\n",
    "            else:\n",
    "                # upload those data not in dataset\n",
    "                df = pd.read_csv(f'{csv_file}{file}')\n",
    "                query = f\"\"\"\n",
    "                SELECT *\n",
    "                FROM `{PROJECT_ID}.{dataset_id}.{table_id}`\n",
    "                \"\"\"\n",
    "                data_exist = client.query(query, project=PROJECT_ID).to_dataframe()\n",
    "                data_exist = data_exist['Your key column name of survey that can check the uniqueness'].tolist()\n",
    "                df = df.loc[~(df['Your key column name of survey that can check the uniqueness'].isin(data_exist)),]\n",
    "                df.to_csv(f'{csv_file}{file}', index = False)\n",
    "                if len(df.index) == 0:\n",
    "                    print(\"Loaded 0 rows into {}: {}.\".format(dataset_id, table_id))\n",
    "                    continue\n",
    "            \n",
    "            ## tell the client everything it needs to know to upload our csv\n",
    "            dataset_ref = client.dataset(dataset_id)\n",
    "            table_ref = dataset_ref.table(table_id)\n",
    "            job_config = bigquery.LoadJobConfig()\n",
    "            job_config.source_format = bigquery.SourceFormat.CSV\n",
    "            job_config.autodetect = True\n",
    "            \n",
    "            \n",
    "            ## load the csv into bigquery\n",
    "            with open(filename, \"rb\") as source_file:\n",
    "                job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "            job.result()  # Waits for table load to complete.\n",
    "\n",
    "            ## looks like everything worked :)\n",
    "            print(\"Loaded {} rows into {}: {}.\".format(job.output_rows, dataset_id, table_id))\n",
    "    \n",
    "    print(\"Done\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e232c9c",
   "metadata": {},
   "source": [
    "## 2. Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPLOAD\n",
    "GCP_upload(PROJECT_ID, csv_path, dataset_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "273aa01e667171dc7eb4f2be461b95807a6349de7b6269aab13a44c761e3f4be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
